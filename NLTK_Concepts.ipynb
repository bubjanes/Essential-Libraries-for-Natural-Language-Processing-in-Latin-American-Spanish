{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Natural Language Tool Kit (version 3.4)\n",
    "### Pros: basic preprocessing tools, tokenizing, and stemming work well, and fast; many corpora are available and in multiple languages, including Spanish\n",
    "### Cons: NLTK fails with more advanced processing in Spanish, such as POS tagging, chunking, named entity reckognition, and text classification\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import europarl_raw\n",
    "from nltk.tokenize import PunktSentenceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#DOWNLOADS; not necessary if already loaded\n",
    "#Also, we will use the NLTK Spanish language corpus from the European Parliament and matplotlib for visualization.\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start with a message from our dataset\n",
    "ex_message = \"Sres de organización centro: por favor necesito de forma urgente que se informe al sr responsable de la grúa, que se me envíe el recibo de pago por mail o por la vía que le sea más conveniente. Necesito presentarla a mi lugar de trabajo. De haber sabido que no tenía recibo, no abonaba. Siempre actuó de buena fe. Hasta soporte ni sólo el riesgo de vida de mi mascota de 5 kilos, si no el hecho de que el sr en cuestión hablaba permanentemente por celular de cuestiones personales. Debo enviar pruebas de esto también? Gracias\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TOKENIZE (good)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sres de organización centro: por favor necesito de forma urgente que se informe al sr responsable de la grúa, que se me envíe el recibo de pago por mail o por la vía que le sea más conveniente.',\n",
       " 'Necesito presentarla a mi lugar de trabajo.',\n",
       " 'De haber sabido que no tenía recibo, no abonaba.',\n",
       " 'Siempre actuó de buena fe.',\n",
       " 'Hasta soporte ni sólo el riesgo de vida de mi mascota de 5 kilos, si no el hecho de que el sr en cuestión hablaba permanentemente por celular de cuestiones personales.',\n",
       " 'Debo enviar pruebas de esto también?',\n",
       " 'Gracias']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = sent_tokenize(ex_message)\n",
    "sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sres',\n",
       " 'de',\n",
       " 'organización',\n",
       " 'centro',\n",
       " ':',\n",
       " 'por',\n",
       " 'favor',\n",
       " 'necesito',\n",
       " 'de',\n",
       " 'forma',\n",
       " 'urgente',\n",
       " 'que',\n",
       " 'se',\n",
       " 'informe',\n",
       " 'al',\n",
       " 'sr',\n",
       " 'responsable',\n",
       " 'de',\n",
       " 'la',\n",
       " 'grúa',\n",
       " ',',\n",
       " 'que',\n",
       " 'se',\n",
       " 'me',\n",
       " 'envíe',\n",
       " 'el',\n",
       " 'recibo',\n",
       " 'de',\n",
       " 'pago',\n",
       " 'por',\n",
       " 'mail',\n",
       " 'o',\n",
       " 'por',\n",
       " 'la',\n",
       " 'vía',\n",
       " 'que',\n",
       " 'le',\n",
       " 'sea',\n",
       " 'más',\n",
       " 'conveniente',\n",
       " '.',\n",
       " 'Necesito',\n",
       " 'presentarla',\n",
       " 'a',\n",
       " 'mi',\n",
       " 'lugar',\n",
       " 'de',\n",
       " 'trabajo',\n",
       " '.',\n",
       " 'De',\n",
       " 'haber',\n",
       " 'sabido',\n",
       " 'que',\n",
       " 'no',\n",
       " 'tenía',\n",
       " 'recibo',\n",
       " ',',\n",
       " 'no',\n",
       " 'abonaba',\n",
       " '.',\n",
       " 'Siempre',\n",
       " 'actuó',\n",
       " 'de',\n",
       " 'buena',\n",
       " 'fe',\n",
       " '.',\n",
       " 'Hasta',\n",
       " 'soporte',\n",
       " 'ni',\n",
       " 'sólo',\n",
       " 'el',\n",
       " 'riesgo',\n",
       " 'de',\n",
       " 'vida',\n",
       " 'de',\n",
       " 'mi',\n",
       " 'mascota',\n",
       " 'de',\n",
       " '5',\n",
       " 'kilos',\n",
       " ',',\n",
       " 'si',\n",
       " 'no',\n",
       " 'el',\n",
       " 'hecho',\n",
       " 'de',\n",
       " 'que',\n",
       " 'el',\n",
       " 'sr',\n",
       " 'en',\n",
       " 'cuestión',\n",
       " 'hablaba',\n",
       " 'permanentemente',\n",
       " 'por',\n",
       " 'celular',\n",
       " 'de',\n",
       " 'cuestiones',\n",
       " 'personales',\n",
       " '.',\n",
       " 'Debo',\n",
       " 'enviar',\n",
       " 'pruebas',\n",
       " 'de',\n",
       " 'esto',\n",
       " 'también',\n",
       " '?',\n",
       " 'Gracias']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = word_tokenize(ex_message)\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STOPWORDS (good)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313  total stopwords\n",
      "['de', 'la', 'que', 'el', 'en', 'y', 'a', 'los', 'del', 'se'] ...\n"
     ]
    }
   ],
   "source": [
    "stop_words = stopwords.words(\"spanish\")\n",
    "print(len(stop_words),' total stopwords')\n",
    "print(stop_words[:10],'...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before removing stopwords:  107  words\n",
      "after:  61  words\n"
     ]
    }
   ],
   "source": [
    "#remove stopwords\n",
    "filtered_sentence = []\n",
    "print('before removing stopwords: ',len(words),' words')\n",
    "for w in words:\n",
    "    if w not in stop_words:\n",
    "        filtered_sentence.append(w)\n",
    "print('after: ',len(filtered_sentence),' words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after:  61  words\n"
     ]
    }
   ],
   "source": [
    "#the famous one-liner: does the same as above\n",
    "filtered_sentence = [w for w in words if not w in stop_words]\n",
    "print('after: ',len(filtered_sentence),' words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEMMING (good)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = SnowballStemmer('spanish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sres\n",
      "organizaci\n",
      "centr\n",
      ":\n",
      "favor\n",
      "necesit\n",
      "form\n",
      "urgent\n",
      "inform\n",
      "sr\n",
      "responsabl\n",
      "gru\n",
      ",\n",
      "envi\n",
      "recib\n",
      "pag\n",
      "mail\n",
      "via\n",
      "convenient\n",
      ".\n",
      "necesit\n",
      "presentarl\n",
      "lug\n",
      "trabaj\n",
      ".\n",
      "de\n",
      "hab\n",
      "sab\n",
      "recib\n",
      ",\n",
      "abon\n",
      ".\n",
      "siempr\n",
      "actu\n",
      "buen\n",
      "fe\n",
      ".\n",
      "hast\n",
      "soport\n",
      "sol\n",
      "riesg\n",
      "vid\n",
      "mascot\n",
      "5\n",
      "kil\n",
      ",\n",
      "si\n",
      "hech\n",
      "sr\n",
      "cuestion\n",
      "habl\n",
      "permanent\n",
      "celul\n",
      "cuestion\n",
      "personal\n",
      ".\n",
      "deb\n",
      "envi\n",
      "prueb\n",
      "?\n",
      "graci\n"
     ]
    }
   ],
   "source": [
    "for s in filtered_sentence:\n",
    "    print(ss.stem(s[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LEMMATIZING (not good)\n",
    "#### lemmatizer.lemmatize('word', pos='n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quiso\n",
      "gatas\n",
      "want\n",
      "good\n",
      "cat\n",
      "goose\n"
     ]
    }
   ],
   "source": [
    "#Spanish\n",
    "lemma_one = lemmatizer.lemmatize('quiso', pos='v')\n",
    "lemma_two = lemmatizer.lemmatize('gatas')\n",
    "#English works!\n",
    "lemma_three = lemmatizer.lemmatize('wanted', pos='v')\n",
    "lemma_four = lemmatizer.lemmatize('better', pos='a')\n",
    "lemma_five = lemmatizer.lemmatize('cats')\n",
    "lemma_six = lemmatizer.lemmatize('geese')\n",
    "print(lemma_one)\n",
    "print(lemma_two)\n",
    "print(lemma_three)\n",
    "print(lemma_four)\n",
    "print(lemma_five)\n",
    "print(lemma_six)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing a Spanish language corpus for training a tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here we import the NLTK Spanish language corpus from the European Parliament to train for part of speech and entity reckognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<EuroparlCorpusReader in '/Users/brandonjanes/nltk_data/corpora/europarl_raw/spanish'>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spanish = europarl_raw.spanish\n",
    "spanish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "155329\n"
     ]
    }
   ],
   "source": [
    "train_text = spanish.raw('ep-00-01-17.es')\n",
    "print(len(train_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)\n",
    "tokenized = custom_sent_tokenizer.tokenize(ex_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS TAGGING (not good)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            print(tagged)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Sres', 'NNS'), ('de', 'VBP'), ('organización', 'FW'), ('centro', 'NN'), (':', ':'), ('por', 'NN'), ('favor', 'NN'), ('necesito', 'FW'), ('de', 'FW'), ('forma', 'FW'), ('urgente', 'JJ'), ('que', 'NN'), ('se', 'JJ'), ('informe', 'NN'), ('al', 'NN'), ('sr', 'NN'), ('responsable', 'JJ'), ('de', 'FW'), ('la', 'FW'), ('grúa', 'FW'), (',', ','), ('que', 'FW'), ('se', 'FW'), ('me', 'PRP'), ('envíe', 'VBP'), ('el', 'JJ'), ('recibo', 'NN'), ('de', 'IN'), ('pago', 'FW'), ('por', 'JJ'), ('mail', 'NN'), ('o', 'NN'), ('por', 'NN'), ('la', 'NN'), ('vía', 'FW'), ('que', 'NN'), ('le', 'FW'), ('sea', 'NN'), ('más', 'NN'), ('conveniente', 'NN'), ('.', '.')]\n",
      "[('Necesito', 'NNP'), ('presentarla', 'VBZ'), ('a', 'DT'), ('mi', 'JJ'), ('lugar', 'NN'), ('de', 'IN'), ('trabajo', 'NN'), ('.', '.')]\n",
      "[('De', 'NNP'), ('haber', 'NNP'), ('sabido', 'NN'), ('que', 'NN'), ('no', 'DT'), ('tenía', 'NN'), ('recibo', 'NN'), (',', ','), ('no', 'DT'), ('abonaba', 'NN'), ('.', '.')]\n",
      "[('Siempre', 'NNP'), ('actuó', 'NN'), ('de', 'FW'), ('buena', 'FW'), ('fe', 'NN'), ('.', '.')]\n",
      "[('Hasta', 'NNP'), ('soporte', 'NN'), ('ni', 'NN'), ('sólo', 'NN'), ('el', 'FW'), ('riesgo', 'NN'), ('de', 'FW'), ('vida', 'FW'), ('de', 'FW'), ('mi', 'FW'), ('mascota', 'FW'), ('de', 'FW'), ('5', 'CD'), ('kilos', 'NN'), (',', ','), ('si', 'VBZ'), ('no', 'DT'), ('el', 'JJ'), ('hecho', 'NN'), ('de', 'IN'), ('que', 'FW'), ('el', 'FW'), ('sr', 'FW'), ('en', 'FW'), ('cuestión', 'NN'), ('hablaba', 'NN'), ('permanentemente', 'NN'), ('por', 'FW'), ('celular', 'NN'), ('de', 'IN'), ('cuestiones', 'NNS'), ('personales', 'NNS'), ('.', '.')]\n",
      "[('Debo', 'NNP'), ('enviar', 'JJ'), ('pruebas', 'NN'), ('de', 'IN'), ('esto', 'FW'), ('también', 'NN'), ('?', '.')]\n",
      "[('Gracias', 'NNP')]\n"
     ]
    }
   ],
   "source": [
    "#NLTK performs poorly with Spanish language text \n",
    "#POS TAG LIST below\n",
    "process_content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#POS TAG LIST: DO NOT RUN THIS BLOCK\n",
    "CC\tcoordinating conjunction\n",
    "CD\tcardinal digit\n",
    "DT\tdeterminer\n",
    "EX\texistential there (like: \"there is\" ... think of it like \"there exists\")\n",
    "FW\tforeign word\n",
    "IN\tpreposition/subordinating conjunction\n",
    "JJ\tadjective\t'big'\n",
    "JJR\tadjective, comparative\t'bigger'\n",
    "JJS\tadjective, superlative\t'biggest'\n",
    "LS\tlist marker\t1)\n",
    "MD\tmodal\tcould, will\n",
    "NN\tnoun, singular 'desk'\n",
    "NNS\tnoun plural\t'desks'\n",
    "NNP\tproper noun, singular\t'Harrison'\n",
    "NNPS\tproper noun, plural\t'Americans'\n",
    "PDT\tpredeterminer\t'all the kids'\n",
    "POS\tpossessive ending\tparent's\n",
    "PRP\tpersonal pronoun\tI, he, she\n",
    "PRP$\tpossessive pronoun\tmy, his, hers\n",
    "RB\tadverb\tvery, silently,\n",
    "RBR\tadverb, comparative\tbetter\n",
    "RBS\tadverb, superlative\tbest\n",
    "RP\tparticle\tgive up\n",
    "TO\tto\tgo 'to' the store.\n",
    "UH\tinterjection\terrrrrrrrm\n",
    "VB\tverb, base form\ttake\n",
    "VBD\tverb, past tense\ttook\n",
    "VBG\tverb, gerund/present participle\ttaking\n",
    "VBN\tverb, past participle\ttaken\n",
    "VBP\tverb, sing. present, non-3d\ttake\n",
    "VBZ\tverb, 3rd person sing. present\ttakes\n",
    "WDT\twh-determiner\twhich\n",
    "WP\twh-pronoun\twho, what\n",
    "WP$\tpossessive wh-pronoun\twhose\n",
    "WRB\twh-abverb\twhere, when"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking (not good)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using regular expressions, we can search for phrases in the form: verb + proper noun\n",
    "#Using matplotlib we will also create a dependency tree for each chunk(sentence). Visualization only works if using Jupyter Notebook, but careful: opens several windows.\n",
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            chunkGram = \"\"\"Chunk: {<VB.?>*<NNP>}\"\"\"\n",
    "            chunkParser = nltk.RegexpParser(chunkGram)\n",
    "            chunked = chunkParser.parse(tagged)\n",
    "            \n",
    "            print(chunked)\n",
    "            chunked.draw()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  Sres/NNS\n",
      "  de/VBP\n",
      "  organización/FW\n",
      "  centro/NN\n",
      "  :/:\n",
      "  por/NN\n",
      "  favor/NN\n",
      "  necesito/FW\n",
      "  de/FW\n",
      "  forma/FW\n",
      "  urgente/JJ\n",
      "  que/NN\n",
      "  se/JJ\n",
      "  informe/NN\n",
      "  al/NN\n",
      "  sr/NN\n",
      "  responsable/JJ\n",
      "  de/FW\n",
      "  la/FW\n",
      "  grúa/FW\n",
      "  ,/,\n",
      "  que/FW\n",
      "  se/FW\n",
      "  me/PRP\n",
      "  envíe/VBP\n",
      "  el/JJ\n",
      "  recibo/NN\n",
      "  de/IN\n",
      "  pago/FW\n",
      "  por/JJ\n",
      "  mail/NN\n",
      "  o/NN\n",
      "  por/NN\n",
      "  la/NN\n",
      "  vía/FW\n",
      "  que/NN\n",
      "  le/FW\n",
      "  sea/NN\n",
      "  más/NN\n",
      "  conveniente/NN\n",
      "  ./.)\n",
      "(S\n",
      "  (Chunk Necesito/NNP)\n",
      "  presentarla/VBZ\n",
      "  a/DT\n",
      "  mi/JJ\n",
      "  lugar/NN\n",
      "  de/IN\n",
      "  trabajo/NN\n",
      "  ./.)\n",
      "(S\n",
      "  (Chunk De/NNP)\n",
      "  (Chunk haber/NNP)\n",
      "  sabido/NN\n",
      "  que/NN\n",
      "  no/DT\n",
      "  tenía/NN\n",
      "  recibo/NN\n",
      "  ,/,\n",
      "  no/DT\n",
      "  abonaba/NN\n",
      "  ./.)\n",
      "(S (Chunk Siempre/NNP) actuó/NN de/FW buena/FW fe/NN ./.)\n",
      "(S\n",
      "  (Chunk Hasta/NNP)\n",
      "  soporte/NN\n",
      "  ni/NN\n",
      "  sólo/NN\n",
      "  el/FW\n",
      "  riesgo/NN\n",
      "  de/FW\n",
      "  vida/FW\n",
      "  de/FW\n",
      "  mi/FW\n",
      "  mascota/FW\n",
      "  de/FW\n",
      "  5/CD\n",
      "  kilos/NN\n",
      "  ,/,\n",
      "  si/VBZ\n",
      "  no/DT\n",
      "  el/JJ\n",
      "  hecho/NN\n",
      "  de/IN\n",
      "  que/FW\n",
      "  el/FW\n",
      "  sr/FW\n",
      "  en/FW\n",
      "  cuestión/NN\n",
      "  hablaba/NN\n",
      "  permanentemente/NN\n",
      "  por/FW\n",
      "  celular/NN\n",
      "  de/IN\n",
      "  cuestiones/NNS\n",
      "  personales/NNS\n",
      "  ./.)\n",
      "(S\n",
      "  (Chunk Debo/NNP)\n",
      "  enviar/JJ\n",
      "  pruebas/NN\n",
      "  de/IN\n",
      "  esto/FW\n",
      "  también/NN\n",
      "  ?/.)\n",
      "(S (Chunk Gracias/NNP))\n"
     ]
    }
   ],
   "source": [
    "#lots of errors because of Spanish\n",
    "process_content()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name Entity Reckognition (bad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Zero percent accuracy (the abbreviation GPE refers to geographical location... nada que ver)\n",
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            entities = nltk.ne_chunk(tagged)\n",
    "            print(entities)\n",
    "    except Exception as e:\n",
    "        print(str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  Sres/NNS\n",
      "  de/VBP\n",
      "  organización/FW\n",
      "  centro/NN\n",
      "  :/:\n",
      "  por/NN\n",
      "  favor/NN\n",
      "  necesito/FW\n",
      "  de/FW\n",
      "  forma/FW\n",
      "  urgente/JJ\n",
      "  que/NN\n",
      "  se/JJ\n",
      "  informe/NN\n",
      "  al/NN\n",
      "  sr/NN\n",
      "  responsable/JJ\n",
      "  de/FW\n",
      "  la/FW\n",
      "  grúa/FW\n",
      "  ,/,\n",
      "  que/FW\n",
      "  se/FW\n",
      "  me/PRP\n",
      "  envíe/VBP\n",
      "  el/JJ\n",
      "  recibo/NN\n",
      "  de/IN\n",
      "  pago/FW\n",
      "  por/JJ\n",
      "  mail/NN\n",
      "  o/NN\n",
      "  por/NN\n",
      "  la/NN\n",
      "  vía/FW\n",
      "  que/NN\n",
      "  le/FW\n",
      "  sea/NN\n",
      "  más/NN\n",
      "  conveniente/NN\n",
      "  ./.)\n",
      "(S\n",
      "  (GPE Necesito/NNP)\n",
      "  presentarla/VBZ\n",
      "  a/DT\n",
      "  mi/JJ\n",
      "  lugar/NN\n",
      "  de/IN\n",
      "  trabajo/NN\n",
      "  ./.)\n",
      "(S\n",
      "  (PERSON De/NNP)\n",
      "  haber/NNP\n",
      "  sabido/NN\n",
      "  que/NN\n",
      "  no/DT\n",
      "  tenía/NN\n",
      "  recibo/NN\n",
      "  ,/,\n",
      "  no/DT\n",
      "  abonaba/NN\n",
      "  ./.)\n",
      "(S (GPE Siempre/NNP) actuó/NN de/FW buena/FW fe/NN ./.)\n",
      "(S\n",
      "  (GPE Hasta/NNP)\n",
      "  soporte/NN\n",
      "  ni/NN\n",
      "  sólo/NN\n",
      "  el/FW\n",
      "  riesgo/NN\n",
      "  de/FW\n",
      "  vida/FW\n",
      "  de/FW\n",
      "  mi/FW\n",
      "  mascota/FW\n",
      "  de/FW\n",
      "  5/CD\n",
      "  kilos/NN\n",
      "  ,/,\n",
      "  si/VBZ\n",
      "  no/DT\n",
      "  el/JJ\n",
      "  hecho/NN\n",
      "  de/IN\n",
      "  que/FW\n",
      "  el/FW\n",
      "  sr/FW\n",
      "  en/FW\n",
      "  cuestión/NN\n",
      "  hablaba/NN\n",
      "  permanentemente/NN\n",
      "  por/FW\n",
      "  celular/NN\n",
      "  de/IN\n",
      "  cuestiones/NNS\n",
      "  personales/NNS\n",
      "  ./.)\n",
      "(S (GPE Debo/NNP) enviar/JJ pruebas/NN de/IN esto/FW también/NN ?/.)\n",
      "(S (GPE Gracias/NNP))\n"
     ]
    }
   ],
   "source": [
    "process_content()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONCLUSION: NLTK is great for basic preprocessing and its extensive corpora, but we cannot take advantage of the higher level processing (i.e. POS tags, NER) because, I believe, the library is not trained for the Spanish language.\n",
    "### One option would be to train a tagger to reckognize parts of speech and named entities in the Spanish language text, or find a 'pre-trained' Spanish text, such as http://www.lsi.upc.edu/~nlp/wikicorpus/. Other modules already exist with more Spanish language capability, such as spaCy and textacy. Perhaps it is best to use them for higher order preprocessing in Spanish. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
