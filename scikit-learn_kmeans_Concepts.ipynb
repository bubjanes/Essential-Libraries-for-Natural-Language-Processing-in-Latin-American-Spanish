{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scikit-learn and kmeans (version 0.20.2)\n",
    "## this notebook uses nltk (version 3.4) for preprocessing; scikit learn for ML; k-means for clustering\n",
    "## using the data set from our client, this is our first attempt to apply ML algorithms and clustering. The results are not great. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "from sklearn import feature_extraction\n",
    "import mpld3\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mpld3 in /anaconda3/envs/diplodatos-ayv/lib/python3.5/site-packages (0.3)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install mpld3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"spanish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparar funciones de procesamiento de texto\n",
    "def tokenize_and_stem(text):\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "            \n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('mattermost_running.csv', sep=',', parse_dates=['creation_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(287,)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = dataset.text.dropna()\n",
    "text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "palabras = []\n",
    "\n",
    "for i in text:\n",
    "    if i is float:\n",
    "        continue\n",
    "    if len(i) < 2:\n",
    "        continue\n",
    "    if (re.search(r'\\d', i)):\n",
    "        continue\n",
    "    if i.startswith('http') :\n",
    "        continue       \n",
    "    p = tokenize_and_stem(i)\n",
    "    palabras.extend(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lista de \"stopwords\"\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = nltk.corpus.stopwords.words('spanish')\n",
    "\n",
    "stopwords.append('https')\n",
    "stopwords.append('rt')\n",
    "stopwords.append('//t.co/86i0lev9kv')\n",
    "stopwords.append('hola')\n",
    "stopwords.append('Hola')\n",
    "\n",
    "f_text = [word for word in palabras if word not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     words\n",
      "0      hol\n",
      "1  necesit\n",
      "2    conoc\n",
      "3     sobr\n",
      "4    preci\n",
      "5    segur\n",
      "6      aut\n",
      "7    prueb\n",
      "8      hol\n",
      "9   client\n"
     ]
    }
   ],
   "source": [
    "vocab_frame = pd.DataFrame({'words': f_text}, index = range(len(f_text)))\n",
    "print(vocab_frame[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/diplodatos-ayv/lib/python3.5/site-packages/sklearn/feature_extraction/text.py:286: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hol', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosostr', 'vuestr'] not in stop_words.\n",
      "  sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La matrix tiene 287 filas (documentos) y 17 columnas (palabras)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compute the term frequency-inverse document frequency matrix\n",
    "tfidf_vectorizer = TfidfVectorizer(binary=True, max_df=.95,\n",
    "                                 min_df=15, stop_words=stopwords,\n",
    "                                 use_idf=True, tokenizer=tokenize_and_stem, ngram_range=(1,5))\n",
    "\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(text.astype('U'))\n",
    "print(\"La matrix tiene %i filas (documentos) y %i columnas (palabras)\\n\" % tfidf_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hay en total 17 palabras:\n",
      "\n",
      "['aut', 'buen', 'com', 'envi', 'graci', 'hac', 'hol', 'necesit', 'pag', 'par', 'pas', 'pued', 'q', 'quier', 'sab', 'segur', 'si']\n"
     ]
    }
   ],
   "source": [
    "terms = tfidf_vectorizer.get_feature_names()\n",
    "print(\"Hay en total %i palabras:\\n\" % len(terms))\n",
    "print(terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters = 7\n",
    "km = KMeans(n_clusters=num_clusters)\n",
    "km.fit(tfidf_matrix)\n",
    "clusters = km.labels_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El cluster 0 tiene 16 elementos\n",
      "El cluster 1 tiene 161 elementos\n",
      "El cluster 2 tiene 29 elementos\n",
      "El cluster 3 tiene 21 elementos\n",
      "El cluster 4 tiene 18 elementos\n",
      "El cluster 5 tiene 24 elementos\n",
      "El cluster 6 tiene 18 elementos\n"
     ]
    }
   ],
   "source": [
    "# Recuento del nÃºmero de elementos en cada cluster\n",
    "for i in range(num_clusters):\n",
    "    print ('El cluster %i tiene %i elementos' % (i, clusters.count(i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = 1 - cosine_similarity(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per cluster:\n",
      "\n",
      "[[ Cluster 0 ]]\n",
      "\n",
      "   WORDS /// segur / hac / sab / si / pag / aut / \n",
      "\n",
      "\n",
      "[[ Cluster 1 ]]\n",
      "\n",
      "   WORDS /// graci / necesit / pag / si / aut / sab / \n",
      "\n",
      "\n",
      "[[ Cluster 2 ]]\n",
      "\n",
      "   WORDS /// envi / pag / quier / si / buen / par / \n",
      "\n",
      "\n",
      "[[ Cluster 3 ]]\n",
      "\n",
      "   WORDS /// pued / pas / com / quier / hac / pag / \n",
      "\n",
      "\n",
      "[[ Cluster 4 ]]\n",
      "\n",
      "   WORDS /// q / aut / pas / par / quier / pued / \n",
      "\n",
      "\n",
      "[[ Cluster 5 ]]\n",
      "\n",
      "   WORDS /// par / segur / quier / com / pas / necesit / \n",
      "\n",
      "\n",
      "[[ Cluster 6 ]]\n",
      "\n",
      "   WORDS /// hol / buen / aut / segur / par / pag / \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Top terms per cluster:\")\n",
    "print()\n",
    "#sort cluster centers by proximity to centroid\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1]     \n",
    "        \n",
    "for i in range(num_clusters):\n",
    "    print(\"[[ Cluster %d ]]\" % i, end='\\n\\n')\n",
    "    \n",
    "    print(\"   WORDS /// \", end='')\n",
    "    \n",
    "    for ind in order_centroids[i, :6]: #replace 6 with n words per cluster\n",
    "        print(terms[ind], end=' / ')\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
