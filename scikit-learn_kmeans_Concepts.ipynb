{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering with scikit-learn and kmeans \n",
    "## NLTK for preprocessing; scikit learn for vectorizing, matrix; k-means for clustering\n",
    "## This is our first attempt to apply ML algorithms with the data set from our client. The results are not great. \n",
    "scikit learn (version 0.20.2); nltk (version 3.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import mpld3\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn import feature_extraction\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mpld3 in /anaconda3/envs/diplodatos-ayv/lib/python3.5/site-packages (0.3)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install mpld3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"spanish\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtain a tokenizer function to use in our vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparar funciones de procesamiento de texto\n",
    "def tokenize_and_stem(text):\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "            \n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add words to stopwords list that we want the ML to ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lista de \"stopwords\"\n",
    "stopwords = nltk.corpus.stopwords.words('spanish')\n",
    "\n",
    "stopwords.append('https')\n",
    "stopwords.append('rt')\n",
    "stopwords.append('//t.co/86i0lev9kv')\n",
    "stopwords.append('hola')\n",
    "stopwords.append('Hola')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import our raw text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('mattermost_running.csv', sep=',', parse_dates=['creation_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "text = dataset.text.dropna()\n",
    "text.shape\n",
    "print(type(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a vectorizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the term frequency-inverse document frequency matrix\n",
    "tfidf_vectorizer = TfidfVectorizer(binary=True, max_df=.95,\n",
    "                                 min_df=15, stop_words=stopwords,\n",
    "                                 use_idf=True, tokenizer=tokenize_and_stem, ngram_range=(1,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform our vector object into a matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/diplodatos-ayv/lib/python3.5/site-packages/sklearn/feature_extraction/text.py:286: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hol', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosostr', 'vuestr'] not in stop_words.\n",
      "  sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La matrix tiene 287 filas (documentos) y 17 columnas (palabras)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tfidf_matrix = tfidf_vectorizer.fit_transform(text.astype('U'))\n",
    "print(\"La matrix tiene %i filas (documentos) y %i columnas (palabras)\\n\" % tfidf_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hay en total 17 palabras:\n",
      "\n",
      "['aut', 'buen', 'com', 'envi', 'graci', 'hac', 'hol', 'necesit', 'pag', 'par', 'pas', 'pued', 'q', 'quier', 'sab', 'segur', 'si']\n"
     ]
    }
   ],
   "source": [
    "terms = tfidf_vectorizer.get_feature_names()\n",
    "print(\"Hay en total %i palabras:\\n\" % len(terms))\n",
    "print(terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k_means: form clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters = 7\n",
    "km = KMeans(n_clusters=num_clusters)\n",
    "km.fit(tfidf_matrix)\n",
    "clusters = km.labels_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El cluster 0 tiene 146 elementos\n",
      "El cluster 1 tiene 22 elementos\n",
      "El cluster 2 tiene 15 elementos\n",
      "El cluster 3 tiene 24 elementos\n",
      "El cluster 4 tiene 35 elementos\n",
      "El cluster 5 tiene 20 elementos\n",
      "El cluster 6 tiene 25 elementos\n"
     ]
    }
   ],
   "source": [
    "# Recuento del n√∫mero de elementos en cada cluster\n",
    "for i in range(num_clusters):\n",
    "    print ('El cluster %i tiene %i elementos' % (i, clusters.count(i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per cluster:\n",
      "\n",
      "[[ Cluster 0 ]]\n",
      "\n",
      "   WORDS /// graci / pas / hac / com / necesit / quier / \n",
      "\n",
      "\n",
      "[[ Cluster 1 ]]\n",
      "\n",
      "   WORDS /// pued / pag / si / com / aut / hac / \n",
      "\n",
      "\n",
      "[[ Cluster 2 ]]\n",
      "\n",
      "   WORDS /// par / com / hac / necesit / envi / pas / \n",
      "\n",
      "\n",
      "[[ Cluster 3 ]]\n",
      "\n",
      "   WORDS /// hol / buen / aut / par / segur / pag / \n",
      "\n",
      "\n",
      "[[ Cluster 4 ]]\n",
      "\n",
      "   WORDS /// segur / hac / sab / par / quier / pas / \n",
      "\n",
      "\n",
      "[[ Cluster 5 ]]\n",
      "\n",
      "   WORDS /// q / aut / pas / par / quier / pued / \n",
      "\n",
      "\n",
      "[[ Cluster 6 ]]\n",
      "\n",
      "   WORDS /// envi / pag / necesit / quier / si / par / \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Top terms per cluster:\")\n",
    "print()\n",
    "#sort cluster centers by proximity to centroid\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1]     \n",
    "        \n",
    "for i in range(num_clusters):\n",
    "    print(\"[[ Cluster %d ]]\" % i, end='\\n\\n')\n",
    "    \n",
    "    print(\"   WORDS /// \", end='')\n",
    "    \n",
    "    for ind in order_centroids[i, :6]: #replace 6 with n words per cluster\n",
    "        print(terms[ind], end=' / ')\n",
    "    print('\\n\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
